{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# For Line Wrapping the Output in each Cell\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "uF92KhwaTfub"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains cropped face images of 16 people divided into Training and testing. We will train the CNN model using the images in the Training folder and then test the model by using the unseen images from the testing folder, to check if the model is able to recognise the face number of the unseen images or not."
      ],
      "metadata": {
        "id": "-QX2z7b7ExgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/christo070/face-recognition.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMbPVV_OEbOt",
        "outputId": "c52a4d1b-5f9a-42de-a29e-7480d960b7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face-recognition'...\n",
            "remote: Enumerating objects: 350, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 350 (delta 0), reused 264 (delta 0), pack-reused 85\u001b[K\n",
            "Receiving objects: 100% (350/350), 14.40 MiB | 38.70 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "3pMrSQ6EENXE",
        "outputId": "72c798ce-b3b2-4190-94e7-a602fe7e0f64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 244 images belonging to 16 classes.\n",
            "Found 244 images belonging to 16 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'face1': 0,\n",
              " 'face10': 1,\n",
              " 'face11': 2,\n",
              " 'face12': 3,\n",
              " 'face13': 4,\n",
              " 'face14': 5,\n",
              " 'face15': 6,\n",
              " 'face16': 7,\n",
              " 'face2': 8,\n",
              " 'face3': 9,\n",
              " 'face4': 10,\n",
              " 'face5': 11,\n",
              " 'face6': 12,\n",
              " 'face7': 13,\n",
              " 'face8': 14,\n",
              " 'face9': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Deep Learning CNN model to recognize face\n",
        "'''This script uses a database of images and creates CNN model on top of it to test\n",
        "   if the given image is recognized correctly or not'''\n",
        "\n",
        "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
        "\n",
        "# Specifying the folder where images are present\n",
        "TrainingImagePath='/content/face-recognition/Final Training Images'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Understand more about ImageDataGenerator at below link\n",
        "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "\n",
        "# Defining pre-processing transformations on raw images of training data\n",
        "# These hyper parameters helps to generate slightly twisted versions\n",
        "# of the original image, which leads to a better model, since it learns\n",
        "# on the good and bad mix of images\n",
        "train_datagen = ImageDataGenerator(\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# Defining pre-processing transformations on raw images of testing data\n",
        "# No transformations are done on the testing images\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Generating the Training Data\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "# Generating the Testing Data\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        TrainingImagePath,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Printing class labels for each face\n",
        "test_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a mapping for index and face names\n",
        "The above class_index dictionary has face names as keys and the numeric mapping as values. We need to swap it, because the classifier model will return the answer as the numeric mapping and we need to get the face-name out of it.\n",
        "\n",
        "Also, since this is a multi-class classification problem, we are counting the number of unique faces, as that will be used as the number of output neurons in the output layer of fully connected ANN classifier."
      ],
      "metadata": {
        "id": "RP6yAQoPE4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''############ Creating lookup table for all faces ############'''\n",
        "# class_indices have the numeric tag for each face\n",
        "TrainClasses=training_set.class_indices\n",
        " \n",
        "# Storing the face and the numeric tag for future reference\n",
        "ResultMap={}\n",
        "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
        "    ResultMap[faceValue]=faceName\n",
        " \n",
        "# Saving the face map for future reference\n",
        "import pickle\n",
        "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
        "    pickle.dump(ResultMap, fileWriteStream)\n",
        " \n",
        "# The model will give answer as a numeric tag\n",
        "# This mapping will help to get the corresponding face name for it\n",
        "print(\"Mapping of Face and its ID\",ResultMap)\n",
        " \n",
        "# The number of neurons for the output layer is equal to the number of faces\n",
        "OutputNeurons=len(ResultMap)\n",
        "print('\\n The Number of output neurons: ', OutputNeurons)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "F3L6irTCE8gK",
        "outputId": "3b6a120a-4f14-468b-fa99-66b80515e964"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
            "\n",
            " The Number of output neurons:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the CNN face recognition model\n",
        "In the below code snippet, the CNN model have\n",
        "\n",
        "2 hidden layers of convolution\n",
        "2 hidden layers of max pooling\n",
        "1 layer of flattening\n",
        "1 Hidden ANN layer\n",
        "1 output layer with 16-neurons (one for each face)\n",
        "\n",
        "## Some of the important hyperparameters\n",
        "\n",
        "* Filters=32: This number indicates how many filters we are using to look at the image pixels during the convolution step. Some filters may catch sharp edges, some filters may catch color variations some filters may catch outlines, etc. In the end, we get important information from the images. In the first layer the number of filters=32 is commonly used, then increasing the power of 2. Like in the next layer it is 64, in the next layer, it is 128 so on and so forth.\n",
        "* kernel_size=(5,5): This indicates the size of the sliding window during convolution, in this case study we are using 5X5 pixels sliding window.\n",
        "* strides=(1, 1): How fast or slow should the sliding window move during convolution. We are using the lowest setting of 1X1 pixels. Means slide the convolution window of 5X5 (kernal_size) by 1 pixel in the x-axis and 1 pixel in the y-axis until the whole image is scanned.\n",
        "* input_shape=(64,64,3): Images are nothing but matrix of RGB color codes. during our data pre-processing we have compressed the images to 64X64, hence the expected shape is 64X64X3. Means 3 arrays of 64X64, one for RGB colors each.\n",
        "* kernel_initializer=’uniform’: When the Neurons start their computation, some algorithm has to decide the value for each weight. This parameter specifies that. You can choose different values for it like ‘normal’ or ‘glorot_uniform’.\n",
        "* activation=’relu’: This specifies the activation function for the calculations inside each neuron. You can choose values like ‘relu’, ‘tanh’, ‘sigmoid’, etc.\n",
        "* optimizer=’adam’: This parameter helps to find the optimum values of each weight in the neural network. ‘adam’ is one of the most useful optimizers, another one is ‘rmsprop’\n",
        "* batch_size=10: This specifies how many rows will be passed to the Network in one go after which the SSE calculation will begin and the neural network will start adjusting its weights based on the errors.\n",
        "When all the rows are passed in the batches of 10 rows each as specified in this parameter, then we call that 1-epoch. Or one full data cycle. This is also known as mini-batch gradient descent. A small value of batch_size will make the LSTM look at the data slowly, like 2 rows at a time or 4 rows at a time which could lead to overfitting, as compared to a large value like 20 or 50 rows at a time, which will make the LSTM look at the data fast which could lead to underfitting. Hence a proper value must be chosen using hyperparameter tuning.\n",
        "* Epochs=10: The same activity of adjusting weights continues for 10 times, as specified by this parameter. In simple terms, the LSTM looks at the full training data 10 times and adjusts its weights."
      ],
      "metadata": {
        "id": "ab09HgfTFNPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''######################## Create CNN deep learning model ########################'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n",
        "'''Initializing the Convolutional Neural Network'''\n",
        "classifier= Sequential()\n",
        "\n",
        "''' STEP--1 Convolution\n",
        "# Adding the first layer of CNN\n",
        "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
        "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
        "'''\n",
        "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
        "\n",
        "'''# STEP--2 MAX Pooling'''\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
        "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
        "\n",
        "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "'''# STEP--3 FLattening'''\n",
        "classifier.add(Flatten())\n",
        "\n",
        "'''# STEP--4 Fully Connected Neural Network'''\n",
        "classifier.add(Dense(64, activation='relu'))\n",
        "\n",
        "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
        "\n",
        "'''# Compiling the CNN'''\n",
        "# classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
        "\n",
        "###########################################################\n",
        "import time\n",
        "# Measuring the time taken by the model to train\n",
        "StartTime=time.time()\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Starting the model training\n",
        "classifier.fit(\n",
        "                training_set,\n",
        "                steps_per_epoch=int(30/batch_size),\n",
        "                epochs=10,\n",
        "                validation_data=test_set,\n",
        "                validation_steps=int(10/batch_size)\n",
        "              )\n",
        "\n",
        "EndTime=time.time()\n",
        "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "Onx9S3XQEqUI",
        "outputId": "89bccb85-cbc6-46f7-8e5b-31314ec16a2b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 3s 427ms/step - loss: 76.1540 - accuracy: 0.0802 - val_loss: 4.2982 - val_accuracy: 0.1094\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 3s 389ms/step - loss: 3.2358 - accuracy: 0.0991 - val_loss: 2.5925 - val_accuracy: 0.1719\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 3s 386ms/step - loss: 2.6159 - accuracy: 0.2028 - val_loss: 2.3725 - val_accuracy: 0.2812\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 3s 390ms/step - loss: 2.3483 - accuracy: 0.3113 - val_loss: 2.2501 - val_accuracy: 0.3125\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 3s 386ms/step - loss: 1.7610 - accuracy: 0.4764 - val_loss: 1.5486 - val_accuracy: 0.5625\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 3s 392ms/step - loss: 1.2456 - accuracy: 0.6557 - val_loss: 0.4957 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 3s 385ms/step - loss: 1.0665 - accuracy: 0.7358 - val_loss: 0.5069 - val_accuracy: 0.8594\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 3s 387ms/step - loss: 0.4126 - accuracy: 0.8726 - val_loss: 0.2531 - val_accuracy: 0.9062\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 3s 405ms/step - loss: 0.3167 - accuracy: 0.9107 - val_loss: 0.1354 - val_accuracy: 0.9531\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 3s 393ms/step - loss: 0.2302 - accuracy: 0.9245 - val_loss: 0.0366 - val_accuracy: 0.9844\n",
            "###### Total Time Taken:  1 Minutes ######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the CNN classifier on unseen images\n",
        "Using any one of the images from the testing data folder, we can check if the model is able to recognize the face."
      ],
      "metadata": {
        "id": "lL5_w1bGGVD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''########### Making single predictions ###########'''\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "\n",
        "ImagePath='/content/face-recognition/Final Testing Images/face4/3face4.jpg'\n",
        "test_image=tf.keras.utils.load_img(ImagePath,target_size=(64, 64))\n",
        "test_image=tf.keras.utils.img_to_array(test_image)\n",
        "\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "\n",
        "result=classifier.predict(test_image,verbose=0)\n",
        "print(training_set.class_indices)\n",
        "\n",
        "print('####'*10)\n",
        "print('Prediction is: ',ResultMap[np.argmax(result)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "NUa6WXWKGYE8",
        "outputId": "07afb9be-aa77-480a-f9e6-b2812d3f9aa2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'face1': 0, 'face10': 1, 'face11': 2, 'face12': 3, 'face13': 4, 'face14': 5, 'face15': 6, 'face16': 7, 'face2': 8, 'face3': 9, 'face4': 10, 'face5': 11, 'face6': 12, 'face7': 13, 'face8': 14, 'face9': 15}\n",
            "########################################\n",
            "Prediction is:  face4\n"
          ]
        }
      ]
    }
  ]
}